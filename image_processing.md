# Tools that speedups image processing:
- [ ] Use Dataset.map(batched=True). See performance boost [here](https://huggingface.co/learn/nlp-course/en/chapter5/3?fw=pt#the-map-methods-superpowers) (0scroll down to see table). But you need to use num_proc and set batched=True in map(), otherwise it will be slower than default set_transform() [as reported here](https://discuss.huggingface.co/t/using-map-take-7-2x-times-longer-than-set-transform/62285).
- [ ] Adapt torchvision v2 manually. Yes you will get speedup over ViTImageProcessorFast on HuggingFace depsite their implementation also base on torchvision v2.
- [ ] Use TorchAug with num_chunks=1 to deal with any random data augmentation (like RandomResizedCrop) where you are forced to loop through each image in a batch. Based on [TorchAug speed benchmark](https://github.com/juliendenize/torchaug/blob/main/docs/source/include/speed_comparison.md) and given that([HuggingFace is still looping through each sample in a batch](https://github.com/huggingface/transformers/blob/main/src/transformers/image_processing_utils_fast.py#L721-L738), you can already gain 3.58x speedup over HuggingFace's ViTImageProcessingFast just for using TorchAug's RandomResizedCrop alone. A little sidenote, kornia also claims to have speedup like TorchAug, [which is entirely false claim](https://github.com/kornia/kornia/issues/1559).
  
# Why adapt torchvision v2 
From my personal benchmark torchvision v2 vs torchvision v2 on vit-base-patch16-224's image preprocessing workflow I notice 2 things:
1. While data augmentation functions are still more efficient to use them on PIL image than torch tensor, 0.2ms speedup per image.
2. Most significant speedup is whenever you can employ v2.functional, >0.4ms speedup per image. <br>
If these number looks small, scale it up by batch_size * num_training_iter * epochs like say 0.2 * 256 * (dataset_size/256) * 200. As your dataset size grows to the around millions as you want to train a new foundation model or finetune foundation model in a completely new domain. Yea, you easily looking at 33 GPU hours saved. <br>

It's very easy to adapt toward torchvision v2. Admittedly, most hours I've spent was digging for the source code of the 2 most popular image processing implementation for vit-base-patch16-224 to makesure that I write down the correct workflow: [HuggingFace's VitImageProcessingFast](https://github.com/huggingface/transformers/blob/v4.49.0/src/transformers/models/vit/image_processing_vit.py#L152-L283), [Timm's implementation](https://github.com/huggingface/transformers/blob/main/examples/pytorch/image-classification/run_image_classification.py#L337-L362); then reading [How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers](https://arxiv.org/abs/2106.10270) paper to make sure I get the hyperparameters right. Once I trust my code and my hyperparameters, it's literally just change `torch.transforms` -> `torch.transforms.v2` and use `torch.transforms.v2.functional` wherever I can. There are 1-2 heuristics I find though so do recommend reading my image_processing.ipynb rather carefully to avoid potential unpleasant surprise.

# Reality-check
- When HuggingFace adopt ViTImageProcessorFast and deprecate ViTImageProcessor, some state-of-the-art models stubbornly stays ViTImageProcessor (see [here](https://github.com/huggingface/transformers/issues/36193)). Considering how moving from "torchvision v1" -> "torchvision v2" is mostly changing some lines of code like say "transforms.Normalise" -> "v2.functional.normalise". Imo, this is the symptom of overbloated code.
