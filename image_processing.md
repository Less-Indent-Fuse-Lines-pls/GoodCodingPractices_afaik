# Tools and tricks that speedups image processing (x means have personally tested):
- [ ] Use Dataset.map(batched=True). See performance boost [here](https://huggingface.co/learn/nlp-course/en/chapter5/3?fw=pt#the-map-methods-superpowers) (scroll down to see table). But you need to use num_proc and set batched=True in map(), otherwise it will be slower than default set_transform() [as reported here](https://discuss.huggingface.co/t/using-map-take-7-2x-times-longer-than-set-transform/62285).
- [x] Adapt torchvision v2 manually using image_processing.ipynb as the first stepping stone. Yes you will get speedup over ViTImageProcessorFast on HuggingFace depsite their implementation also base on torchvision v2.
- [x] Use TorchAug transform(num_chunks=1, batch_transform=True) to deal with any random data augmentation (like RandomResizedCrop) where you are forced to loop through each image in a batch. Having tested, couldn't make it work as despite having batch_transform advantage on paper, TorchAug not only causes massive tensor discrepancy but also slower than torchvision v1 (see in image_processing.ipynb). Unfortunately, TorchAug's authors have archived their project so I will be on my own tryna fix this problem.
- [x] Kornia also claims to have speedup like TorchAug, [which is entirely false claim](https://github.com/kornia/kornia/issues/1559).
- [x] Save random augmented dataset as batch tensor, mostly to avoid dealing with random data augmentation again when we inevitably rerun another model training.
  
# Findings from benchmark in image_processing.ipynb 
1. Data augmentation is more efficient to use on tensor than PIL image at the cost of tensor SSE (Sum of Squared Error) when comparing to torchvision v1 workflwo: ~0.2ms speedup per image for 1.0175 instead of 9.6444e-06. This has implication to finetuning instability!
2. Speed gains from v2.functional is offset by inefficiency of random data augmentation: Even when I ran the transform on 1 image, overall speedup per image compared to torchvision v1 is only 0.08ms (see full transform benchmark in image_processing.ipynb).
3. Thus, the most speedup is to save random augmented dataset as batch tensor. First time training the model we won't save anything, but on the second time we can save 661.89ms for batch size = 256 (see batch transform benchmark in image_processing.ipynb). Suppose your dataset sizes 6 millions, this will scale up to 661.89/1000 * (dataset_size/batch_size) / 3600 = 4.3 GPU hours per training. For a research project, you will train the model around like 10-15 to debug the model, optimise the code, optimise the parameters and hyperparameters; this further scales to 43-64 GPU hours saved over a research project.

# Reality-check
- When HuggingFace adopt ViTImageProcessorFast and deprecate ViTImageProcessor, some state-of-the-art models stubbornly stays ViTImageProcessor (see [here](https://github.com/huggingface/transformers/issues/36193)). Considering how moving from "torchvision v1" -> "torchvision v2" is mostly changing some lines of code like say "transforms.Normalise" -> "v2.functional.normalise". Imo, this is the symptom of overbloated code. When I first doing this, I needed to spent hours digging at the source code of the 2 popular image processing implementation for vit-base-patch16-224 to makesure that I write down the correct workflow: [HuggingFace's VitImageProcessingFast](https://github.com/huggingface/transformers/blob/v4.49.0/src/transformers/models/vit/image_processing_vit.py#L152-L283) and [Timm's implementation](https://github.com/huggingface/transformers/blob/main/examples/pytorch/image-classification/run_image_classification.py#L337-L362); then reading [How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers](https://arxiv.org/abs/2106.10270) paper to make sure I get the hyperparameters right. This is akin to "literature review". The coding itself is easy, just change `torch.transforms` -> `torch.transforms.v2` and use `torch.transforms.v2.functional` wherever I can.
